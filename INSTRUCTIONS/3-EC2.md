# Ubuntu EC2 instance t2.2xlarge
The instance is the core part of the project. We must differenciate between testing (model training and testing) and production (production ML pipeline).

## Launch and connect to the instance
I suggest to chose a non-free tier instance if you really want to train and deploy your own model. Unfortunately, the t2.micro offered by AWS as free tier, is not good enough to do it. Eventually it will get full just right after installing the dependencies.


**Step 1: Launch an Amazon EC2 Instance**

1. Log in to your AWS Management Console.
2. Navigate to the EC2 Dashboard.

   - Click on "Instances" in the left sidebar.
   - Click the "Launch Instances" button.

3. Choose an Amazon Machine Image (AMI). You can select the latest Ubuntu Server image.

4. Configure the instance details (e.g., number of instances, VPC, subnet, security groups). Ensure that the security group allows SSH access (port 22) and Jupyter Notebook access (port 8888).


5. Configure the security group to allow SSH and Jupyter Notebook access. You can create a custom security group or modify the default one.

6. Review your instance configuration, and click "Launch."

7.  Choose an existing key pair or create a new one to securely access your instance via SSH. Download the private key (.pem) file and store it in a secure location.

8.  Launch the instance.

**Step 2: Connect to the EC2 Instance**

1. Open your terminal or SSH client.

2. Navigate to the directory where your private key (.pem) is located.

3. Use the following command to connect to the EC2 instance, replacing `your-key.pem` and `your-instance-public-ip` with your actual key file and instance details:

   ```bash
   ssh -i your-key.pem ubuntu@your-instance-public-ip
   ```

   Example:
   ```bash
   ssh -i my-key.pem ubuntu@123.456.78.90
   ```

**Step 3: Install Dependencies and Set Up PySpark with Jupyter Notebook**

1. Update the package list on the EC2 instance:

   ```bash
   sudo apt-get update
   ```

2. Install Java (required for Spark):

   ```bash
   sudo apt-get install openjdk-8-jdk -y
   ```

3. Download and install Apache Spark. You can get the latest link from the [Apache Spark website](https://spark.apache.org/downloads.html):

   ```bash
   wget https://apache.mirror.digitalpacific.com.au/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
   tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz
   sudo mv spark-3.1.2-bin-hadoop3.2 /usr/local/spark
   ```
In my case I'm running the spark 3.5.0 (published on September 2023!). Choose your prefered version and modify the above code accordingly.

4. Add Spark's bin directory to your PATH:

   ```bash
   echo 'export PATH=$PATH:/usr/local/spark/bin' >> ~/.bashrc
   source ~/.bashrc
   ```

5. Install Python 3 and pip:

   ```bash
   sudo apt-get install python3 python3-pip -y
   ```

6. Install Jupyter Notebook and the PySpark library:

   ```bash
   sudo pip3 install jupyter
   sudo pip3 install pyspark
   ```

7. Start a Jupyter Notebook server:

   ```bash
   jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser
   ```
You will be prompted to the following screen if correctly setup:

![Alt text](/images/jupyter-ubuntu.png)

8. Access Jupyter Notebook from your local web browser by navigating to `http://your-instance-public-ip:8888`. You will need to enter the token displayed in the terminal to log in.

Now you have a working Jupyter Notebook environment with PySpark on your EC2 instance, and you can start creating and running Spark applications using Python.





