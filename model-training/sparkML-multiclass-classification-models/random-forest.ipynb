{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# sparkML\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# sparkSQL\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# other\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk functions --> Necessary to remove stopwords and more to get better accuracy in our NLP ML model\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TextClassification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def preprocess_text(text):\n",
    "    '''Remove stop words, tokenize and clean the data from the title column. Original: |Cis Men of Reddit, if you were a woman, what would you like about Men?  --> PROCESSED: men reddit woman would like men''' \n",
    "    tokens = nltk.word_tokenize(text.lower(), language=\"english\")\n",
    "    tokens = [word for word in tokens if word.isalnum() and len(word) > 1]\n",
    "    tokens = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   topic_name|count|\n",
      "+-------------+-----+\n",
      "|relationships|  140|\n",
      "|        money|  155|\n",
      "|         food|  139|\n",
      "|          job|  142|\n",
      "|         life|  112|\n",
      "|        music|  118|\n",
      "|        media|  122|\n",
      "|        movie|  140|\n",
      "|       sexual|  152|\n",
      "|       health|  145|\n",
      "|          kid|  143|\n",
      "|         game|  126|\n",
      "|         book|  113|\n",
      "|         tech|  116|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled dataset into spark\n",
    "df = spark.read.csv(\"labeled-training-dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# We need a copy of the column, as we are going to tokenize and vektorize the title to better classification\n",
    "df = df.withColumn(\"original_title\", df[\"title\"])\n",
    "\n",
    "# Data Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# Create a list of unique values of the topic_name column in the csv file. Eventually, you will find some rows that contain a really rare character combination, so you avoid it creating specific previous topics.\n",
    "TARGET_CLASSES = [\"money\", \"food\", \"job\", \"life\", \"music\", \"media\", \"movie\", \"sexual\", \"health\", \"kid\", \"game\", \"book\", \"tech\", \"relationships\"]\n",
    "\n",
    "# Filter out the elements that are not falling in any of the classes due to a SyntaxError in the Reddit Sentence --> \"Is calling someone a \"\"plaything\"\" a porn term? If so, what type of porn/ kink content?\",sexual [Look at that combination of double \"\"]\n",
    "df = df.filter(df[\"topic_name\"].isin(TARGET_CLASSES))\n",
    "\n",
    "# Call the preprocess_text function to tokenize and lematize the title of the reddit post\n",
    "preprocess_udf = udf(preprocess_text, StringType())\n",
    "df = df.withColumn(\"title\", preprocess_udf(df[\"title\"]))\n",
    "\n",
    "# Convert the topic_name column to numeric\n",
    "indexer = StringIndexer(inputCol=\"topic_name\", outputCol=\"label\")\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "indexed_df = indexed_df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "\n",
    "# Split the dataset\n",
    "(train_df, test_df) = indexed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# TF-IDF Vectorization --> Classic steps prior to train a Multiclassification model in NLP\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", vocabSize=1500)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100, maxDepth=4, seed=42)\n",
    "\n",
    "# Modify the pipeline to use Random Forest\n",
    "pipeline_rf = Pipeline(stages=[tokenizer, remover, cv, idf, rf])\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "rf_model = pipeline_rf.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.4771\n",
      "Classification Report for Testing Data:\n",
      "Topic: money\n",
      "Precision: 0.1419\n",
      "Recall: 0.8148\n",
      "F1-Score: 0.2418\n",
      "Support: 27\n",
      "\n",
      "Topic: food\n",
      "Precision: 0.5000\n",
      "Recall: 0.4483\n",
      "F1-Score: 0.4727\n",
      "Support: 29\n",
      "\n",
      "Topic: job\n",
      "Precision: 0.9231\n",
      "Recall: 0.4615\n",
      "F1-Score: 0.6154\n",
      "Support: 26\n",
      "\n",
      "Topic: life\n",
      "Precision: 0.7391\n",
      "Recall: 0.7083\n",
      "F1-Score: 0.7234\n",
      "Support: 24\n",
      "\n",
      "Topic: music\n",
      "Precision: 0.7500\n",
      "Recall: 0.4615\n",
      "F1-Score: 0.5714\n",
      "Support: 26\n",
      "\n",
      "Topic: media\n",
      "Precision: 0.8235\n",
      "Recall: 0.6087\n",
      "F1-Score: 0.7000\n",
      "Support: 23\n",
      "\n",
      "Topic: movie\n",
      "Precision: 0.5000\n",
      "Recall: 0.0333\n",
      "F1-Score: 0.0625\n",
      "Support: 30\n",
      "\n",
      "Topic: sexual\n",
      "Precision: 1.0000\n",
      "Recall: 0.2727\n",
      "F1-Score: 0.4286\n",
      "Support: 22\n",
      "\n",
      "Topic: health\n",
      "Precision: 0.7778\n",
      "Recall: 0.8750\n",
      "F1-Score: 0.8235\n",
      "Support: 24\n",
      "\n",
      "Topic: kid\n",
      "Precision: 1.0000\n",
      "Recall: 0.1600\n",
      "F1-Score: 0.2759\n",
      "Support: 25\n",
      "\n",
      "Topic: game\n",
      "Precision: 1.0000\n",
      "Recall: 0.5714\n",
      "F1-Score: 0.7273\n",
      "Support: 21\n",
      "\n",
      "Topic: book\n",
      "Precision: 0.5000\n",
      "Recall: 0.0588\n",
      "F1-Score: 0.1053\n",
      "Support: 17\n",
      "\n",
      "Topic: tech\n",
      "Precision: 0.9167\n",
      "Recall: 0.6875\n",
      "F1-Score: 0.7857\n",
      "Support: 16\n",
      "\n",
      "Topic: relationships\n",
      "Precision: 0.8333\n",
      "Recall: 0.5882\n",
      "F1-Score: 0.6897\n",
      "Support: 17\n",
      "\n",
      "Accuracy: 0.4771\n",
      "Topic: macro avg\n",
      "Precision: 0.7432\n",
      "Recall: 0.4822\n",
      "F1-Score: 0.5159\n",
      "Support: 327\n",
      "\n",
      "Topic: weighted avg\n",
      "Precision: 0.7263\n",
      "Recall: 0.4771\n",
      "F1-Score: 0.5012\n",
      "Support: 327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "accuracy_rf = evaluator.evaluate(predictions)\n",
    "print(\"Random Forest Accuracy: {:.4f}\".format(accuracy_rf))\n",
    "\n",
    "# Classification Report\n",
    "y_true = predictions.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "y_pred = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "report = classification_report(y_true, y_pred, target_names=TARGET_CLASSES, output_dict=True)\n",
    "\n",
    "print(\"Classification Report for Testing Data:\")\n",
    "for topic, metrics in report.items():\n",
    "    if topic == 'accuracy':\n",
    "        print(f\"Accuracy: {metrics:.4f}\")\n",
    "    else:\n",
    "        print(f\"Topic: {topic}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1-score']:.4f}\")\n",
    "        print(f\"Support: {metrics['support']:.0f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
