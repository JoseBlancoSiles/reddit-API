{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd1bc3c-5f21-4a0f-a46e-bef10027ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# sparkML\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# sparkSQL\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# other\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import findspark\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50339473-70ee-4318-9d31-491d3976a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8169b1d0-9244-422a-a1e7-36880d24fe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk functions --> Necessary to remove stopwords and more to get better accuracy in our NLP ML model\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5f7c67-a4b8-4ce9-8f14-91fb37b8232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/19 14:39:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ModelTraining\")\n",
    "\n",
    "spark.config(\"spark.executor.cores\", \"4\")  # Use all 4 available cores per executor\n",
    "spark.config(\"spark.driver.cores\", \"1\")  # Use 1 core for the driver\n",
    "spark.config(\"spark.default.parallelism\", \"4\")\n",
    "\n",
    "spark = spark.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd529166-eee8-40f9-8a10-8053ccde7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for preprocessing\n",
    "def preprocess_text(text):\n",
    "    '''Remove stop words, tokenize and clean the data from the title column. Original: |Cis Men of Reddit, if you were a woman, what would you like about Men?  --> PROCESSED: men reddit woman would like men''' \n",
    "    tokens = nltk.word_tokenize(text.lower(), language=\"english\")\n",
    "    tokens = [word for word in tokens if word.isalnum() and len(word) > 1]\n",
    "    tokens = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b12a0b9-d6f6-4def-8da6-dbe329e386c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the labeled dataset into spark\n",
    "df = spark.read.csv(\"labeled-training-dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# We need a copy of the column, as we are going to tokenize and vektorize the title to better classification\n",
    "df = df.withColumn(\"original_title\", df[\"title\"])\n",
    "\n",
    "# Data Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# Create a list of unique values of the topic_name column in the csv file. Eventually, you will find some rows that contain a really rare character combination, so you avoid it creating specific previous topics.\n",
    "TARGET_CLASSES = [\"money\", \"food\", \"job\", \"life\", \"music\", \"media\", \"movie\", \"sexual\", \"health\", \"kid\", \"game\", \"book\", \"tech\", \"relationships\"]\n",
    "\n",
    "# Filter out the elements that are not falling in any of the classes due to a SyntaxError in the Reddit Sentence --> \"Is calling someone a \"\"plaything\"\" a porn term? If so, what type of porn/ kink content?\",sexual [Look at that combination of double \"\"]\n",
    "df = df.filter(df[\"topic_name\"].isin(TARGET_CLASSES))\n",
    "\n",
    "preprocess_udf = udf(preprocess_text, StringType())\n",
    "df = df.withColumn(\"title\", preprocess_udf(df[\"title\"]))\n",
    "\n",
    "# Convert the topic_name column to numeric\n",
    "indexer = StringIndexer(inputCol=\"topic_name\", outputCol=\"label\")\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "indexed_df = indexed_df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "\n",
    "# Split the dataset\n",
    "(train_df, test_df) = indexed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# TF-IDF Vectorization --> Classic steps prior to train a Multiclassification model in NLP\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", vocabSize=1500)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Modify the pipeline to use Logistic Regression\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lr])\n",
    "\n",
    "# Hyperparameter Tuning for Logistic Regression\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]).addGrid(lr.elasticNetParam, [0.0, 0.1, 0.5]).build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cd6833-a1c2-4fd8-8af9-ea2795c90f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 14:40:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2da3f15-d293-42f6-9dce-30af012bc5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Testing Data:\n",
      "Topic: money\n",
      "Precision: 0.4828\n",
      "Recall: 0.5185\n",
      "F1-Score: 0.5000\n",
      "Support: 27\n",
      "\n",
      "Topic: food\n",
      "Precision: 0.6774\n",
      "Recall: 0.7241\n",
      "F1-Score: 0.7000\n",
      "Support: 29\n",
      "\n",
      "Topic: job\n",
      "Precision: 0.3778\n",
      "Recall: 0.6538\n",
      "F1-Score: 0.4789\n",
      "Support: 26\n",
      "\n",
      "Topic: life\n",
      "Precision: 0.8182\n",
      "Recall: 0.7500\n",
      "F1-Score: 0.7826\n",
      "Support: 24\n",
      "\n",
      "Topic: music\n",
      "Precision: 0.7000\n",
      "Recall: 0.5385\n",
      "F1-Score: 0.6087\n",
      "Support: 26\n",
      "\n",
      "Topic: media\n",
      "Precision: 0.8000\n",
      "Recall: 0.6957\n",
      "F1-Score: 0.7442\n",
      "Support: 23\n",
      "\n",
      "Topic: movie\n",
      "Precision: 0.4286\n",
      "Recall: 0.5000\n",
      "F1-Score: 0.4615\n",
      "Support: 30\n",
      "\n",
      "Topic: sexual\n",
      "Precision: 0.7059\n",
      "Recall: 0.5455\n",
      "F1-Score: 0.6154\n",
      "Support: 22\n",
      "\n",
      "Topic: health\n",
      "Precision: 0.8750\n",
      "Recall: 0.8750\n",
      "F1-Score: 0.8750\n",
      "Support: 24\n",
      "\n",
      "Topic: kid\n",
      "Precision: 0.9048\n",
      "Recall: 0.7600\n",
      "F1-Score: 0.8261\n",
      "Support: 25\n",
      "\n",
      "Topic: game\n",
      "Precision: 0.9444\n",
      "Recall: 0.8095\n",
      "F1-Score: 0.8718\n",
      "Support: 21\n",
      "\n",
      "Topic: book\n",
      "Precision: 0.6667\n",
      "Recall: 0.4706\n",
      "F1-Score: 0.5517\n",
      "Support: 17\n",
      "\n",
      "Topic: tech\n",
      "Precision: 0.8125\n",
      "Recall: 0.8125\n",
      "F1-Score: 0.8125\n",
      "Support: 16\n",
      "\n",
      "Topic: relationships\n",
      "Precision: 0.7059\n",
      "Recall: 0.7059\n",
      "F1-Score: 0.7059\n",
      "Support: 17\n",
      "\n",
      "Accuracy: 0.6636\n",
      "Topic: macro avg\n",
      "Precision: 0.7071\n",
      "Recall: 0.6685\n",
      "F1-Score: 0.6810\n",
      "Support: 327\n",
      "\n",
      "Topic: weighted avg\n",
      "Precision: 0.6939\n",
      "Recall: 0.6636\n",
      "F1-Score: 0.6719\n",
      "Support: 327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "# Classification Report\n",
    "y_true = predictions.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "y_pred = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "report = classification_report(y_true, y_pred, target_names=TARGET_CLASSES, output_dict=True)\n",
    "\n",
    "print(\"Classification Report for Testing Data:\")\n",
    "for topic, metrics in report.items():\n",
    "    if topic == 'accuracy':\n",
    "        print(f\"Accuracy: {metrics:.4f}\")\n",
    "    else:\n",
    "        print(f\"Topic: {topic}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1-score']:.4f}\")\n",
    "        print(f\"Support: {metrics['support']:.0f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c647fc-9855-4d60-ab3c-005d4d5781cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The accuracy is good enough (as tested offline with new data) --> we save the model:\n",
    "model_path = \"trained-model\"\n",
    "cvModel.bestModel.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fc32a2-5314-43db-a545-4f69f7afedbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98008bc1-d363-4b07-9540-327cad674ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
